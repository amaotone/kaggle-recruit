{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amane/.pyenv/versions/3.6.1/envs/compe/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/amane/.pyenv/versions/3.6.1/envs/compe/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/amane/.pyenv/versions/3.6.1/envs/compe/lib/python3.6/site-packages/sklearn/learning_curve.py:22: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn import *\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log1p_beforehand = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import *\n",
    "from datetime import datetime\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "data = {\n",
    "    'tra': pd.read_csv('../input/air_visit_data.csv'),\n",
    "    'as': pd.read_csv('../input/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../input/hpg_store_info.csv'),\n",
    "    'ar': pd.read_csv('../input/air_reserve.csv'),\n",
    "    'hr': pd.read_csv('../input/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../input/store_id_relation.csv'),\n",
    "    'tes': pd.read_csv('../input/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../input/date_info.csv').rename(columns={'calendar_date':'visit_date'})\n",
    "    }\n",
    "\n",
    "if log1p_beforehand:\n",
    "    data['tra']['visitors'] = np.log1p(data['tra']['visitors'])\n",
    "\n",
    "data['hr'] = pd.merge(data['hr'], data['id'], how='inner', on=['hpg_store_id'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    data[df]['visit_datetime'] = pd.to_datetime(data[df]['visit_datetime'])\n",
    "    data[df]['visit_datetime'] = data[df]['visit_datetime'].dt.date\n",
    "    data[df]['reserve_datetime'] = pd.to_datetime(data[df]['reserve_datetime'])\n",
    "    data[df]['reserve_datetime'] = data[df]['reserve_datetime'].dt.date\n",
    "    data[df]['reserve_datetime_diff'] = data[df].apply(lambda r: (r['visit_datetime'] - r['reserve_datetime']).days, axis=1)\n",
    "    tmp1 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].sum().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs1', 'reserve_visitors':'rv1'})\n",
    "    tmp2 = data[df].groupby(['air_store_id','visit_datetime'], as_index=False)[['reserve_datetime_diff', 'reserve_visitors']].mean().rename(columns={'visit_datetime':'visit_date', 'reserve_datetime_diff': 'rs2', 'reserve_visitors':'rv2'})\n",
    "    data[df] = pd.merge(tmp1, tmp2, how='inner', on=['air_store_id','visit_date'])\n",
    "\n",
    "data['tra']['visit_date'] = pd.to_datetime(data['tra']['visit_date'])\n",
    "data['tra']['dow'] = data['tra']['visit_date'].dt.dayofweek\n",
    "data['tra']['year'] = data['tra']['visit_date'].dt.year\n",
    "data['tra']['month'] = data['tra']['visit_date'].dt.month\n",
    "data['tra']['visit_date'] = data['tra']['visit_date'].dt.date\n",
    "\n",
    "data['tes']['visit_date'] = data['tes']['id'].map(lambda x: str(x).split('_')[2])\n",
    "data['tes']['air_store_id'] = data['tes']['id'].map(lambda x: '_'.join(x.split('_')[:2]))\n",
    "data['tes']['visit_date'] = pd.to_datetime(data['tes']['visit_date'])\n",
    "data['tes']['dow'] = data['tes']['visit_date'].dt.dayofweek\n",
    "data['tes']['year'] = data['tes']['visit_date'].dt.year\n",
    "data['tes']['month'] = data['tes']['visit_date'].dt.month\n",
    "data['tes']['visit_date'] = data['tes']['visit_date'].dt.date\n",
    "\n",
    "unique_stores = data['tes']['air_store_id'].unique()\n",
    "stores = pd.concat([pd.DataFrame({'air_store_id': unique_stores, 'dow': [i]*len(unique_stores)}) for i in range(7)], axis=0, ignore_index=True).reset_index(drop=True)\n",
    "\n",
    "#sure it can be compressed...\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].min().rename(columns={'visitors':'min_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].mean().rename(columns={'visitors':'mean_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].median().rename(columns={'visitors':'median_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].max().rename(columns={'visitors':'max_visitors'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow'])\n",
    "tmp = data['tra'].groupby(['air_store_id','dow'], as_index=False)['visitors'].count().rename(columns={'visitors':'count_observations'})\n",
    "stores = pd.merge(stores, tmp, how='left', on=['air_store_id','dow']) \n",
    "\n",
    "stores = pd.merge(stores, data['as'], how='left', on=['air_store_id']) \n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "stores['air_genre_name'] = stores['air_genre_name'].map(lambda x: str(str(x).replace('/',' ')))\n",
    "stores['air_area_name'] = stores['air_area_name'].map(lambda x: str(str(x).replace('-',' ')))\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "for i in range(10):\n",
    "    stores['air_genre_name'+str(i)] = lbl.fit_transform(stores['air_genre_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "    stores['air_area_name'+str(i)] = lbl.fit_transform(stores['air_area_name'].map(lambda x: str(str(x).split(' ')[i]) if len(str(x).split(' '))>i else ''))\n",
    "stores['air_genre_name'] = lbl.fit_transform(stores['air_genre_name'])\n",
    "stores['air_area_name'] = lbl.fit_transform(stores['air_area_name'])\n",
    "\n",
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['day_of_week'] = lbl.fit_transform(data['hol']['day_of_week'])\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "train = pd.merge(data['tra'], data['hol'], how='left', on=['visit_date']) \n",
    "test = pd.merge(data['tes'], data['hol'], how='left', on=['visit_date']) \n",
    "\n",
    "train = pd.merge(train, stores, how='left', on=['air_store_id','dow']) \n",
    "test = pd.merge(test, stores, how='left', on=['air_store_id','dow'])\n",
    "\n",
    "for df in ['ar','hr']:\n",
    "    train = pd.merge(train, data[df], how='left', on=['air_store_id','visit_date']) \n",
    "    test = pd.merge(test, data[df], how='left', on=['air_store_id','visit_date'])\n",
    "\n",
    "train['id'] = train.apply(lambda r: '_'.join([str(r['air_store_id']), str(r['visit_date'])]), axis=1)\n",
    "\n",
    "train['total_reserv_sum'] = train['rv1_x'] + train['rv1_y']\n",
    "train['total_reserv_mean'] = (train['rv2_x'] + train['rv2_y']) / 2\n",
    "train['total_reserv_dt_diff_mean'] = (train['rs2_x'] + train['rs2_y']) / 2\n",
    "\n",
    "test['total_reserv_sum'] = test['rv1_x'] + test['rv1_y']\n",
    "test['total_reserv_mean'] = (test['rv2_x'] + test['rv2_y']) / 2\n",
    "test['total_reserv_dt_diff_mean'] = (test['rs2_x'] + test['rs2_y']) / 2\n",
    "\n",
    "# NEW FEATURES FROM JMBULL\n",
    "train['date_int'] = train['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "test['date_int'] = test['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "train['var_max_lat'] = train['latitude'].max() - train['latitude']\n",
    "train['var_max_long'] = train['longitude'].max() - train['longitude']\n",
    "test['var_max_lat'] = test['latitude'].max() - test['latitude']\n",
    "test['var_max_long'] = test['longitude'].max() - test['longitude']\n",
    "\n",
    "# NEW FEATURES FROM Georgii Vyshnia\n",
    "train['lon_plus_lat'] = train['longitude'] + train['latitude'] \n",
    "test['lon_plus_lat'] = test['longitude'] + test['latitude']\n",
    "\n",
    "lbl = preprocessing.LabelEncoder()\n",
    "train['air_store_id2'] = lbl.fit_transform(train['air_store_id'])\n",
    "test['air_store_id2'] = lbl.transform(test['air_store_id'])\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "def RMSLE(y, pred):\n",
    "    return metrics.mean_squared_error(y, pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 54)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'air_area_name', 'air_genre_name', 'visit_date','visitors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "air_store_id\n",
       "air_00a91d42b08b08d9    3.167414\n",
       "air_0164b9927d20bcc3    2.117522\n",
       "air_0241aa3964b7f861    2.236649\n",
       "air_0328696196e46f18    1.966041\n",
       "air_034a3d5b40d5b1b1    2.457141\n",
       "air_036d4f1ee7285390    3.016290\n",
       "air_0382c794b73b51ad    3.014849\n",
       "air_03963426c9312048    3.466131\n",
       "air_04341b588bde96cd    3.488073\n",
       "air_049f6d5b402a31b2    2.367463\n",
       "air_04cae7c1bc9b2a0b    2.970835\n",
       "air_0585011fa179bcce    1.654938\n",
       "air_05c325d315cc17f5    3.067861\n",
       "air_0647f17b4dc041c8    3.396164\n",
       "air_064e203265ee5753    3.037076\n",
       "air_066f0221b8a4d533    2.472980\n",
       "air_06f95ac5c33aca10    2.973037\n",
       "air_0728814bd98f7367    2.183842\n",
       "air_0768ab3910f7967f    2.547386\n",
       "air_07b314d83059c4d2    3.436534\n",
       "air_07bb665f9cdfbdfb    3.279341\n",
       "air_082908692355165e    3.487676\n",
       "air_083ddc520ea47e1e    2.209605\n",
       "air_0845d8395f30c6bb    2.926955\n",
       "air_084d98859256acf0    2.637420\n",
       "air_0867f7bebad6a649    2.917509\n",
       "air_08ba8cd01b3ba010    2.301373\n",
       "air_08cb3c4ee6cd6a22    2.605300\n",
       "air_08ef81d5b7a0d13f    2.531565\n",
       "air_08f994758a1e76d4    3.122258\n",
       "                          ...   \n",
       "air_f6b2489ccf873c3b    2.803614\n",
       "air_f6bfd27e2e174d16    2.606251\n",
       "air_f6cdaf7b7fdc6d78    2.255556\n",
       "air_f8233ad00755c35c    3.422533\n",
       "air_f85e21e543cf44f2    1.628287\n",
       "air_f88898cd09f40496    2.042773\n",
       "air_f911308e19d64236    3.510842\n",
       "air_f9168b23fdfc1e52    2.418111\n",
       "air_f927b2da69a82341    2.621582\n",
       "air_f957c6d6467d4d90    2.302023\n",
       "air_f96765e800907c77    3.486974\n",
       "air_fa12b40b02fecfd8    2.724231\n",
       "air_fa4ffc9057812fa2    1.818458\n",
       "air_fab092c35776a9b1    2.433249\n",
       "air_fb44f566d4f64a4e    2.725653\n",
       "air_fbadf737162a5ce3    3.002245\n",
       "air_fc477473134e9ae5    2.733659\n",
       "air_fcd4492c83f1c6b9    3.174809\n",
       "air_fcfbdcf7b1f82c6e    3.472546\n",
       "air_fd154088b1de6fa7    1.796471\n",
       "air_fd6aac1043520e83    3.391468\n",
       "air_fdc02ec4a3d21ea4    2.499339\n",
       "air_fdcfef8bd859f650    1.411620\n",
       "air_fe22ef5a9cbef123    3.079709\n",
       "air_fe58c074ec1445ea    3.319302\n",
       "air_fea5dc9594450608    2.572093\n",
       "air_fee8dcf4d619598e    3.217393\n",
       "air_fef9ccb3ba0da2f7    2.227733\n",
       "air_ffcc2d5087e1b476    2.886465\n",
       "air_fff68b929994bfbd    1.672343\n",
       "Name: visitors, Length: 829, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.groupby('air_store_id').visitors.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def target_encode(col):\n",
    "    global train, test\n",
    "    ref = train.loc[~val_id].groupby(col)['visitors'].mean().to_frame(name=col+'_target')\n",
    "    train = pd.merge(left=train, right=ref, left_on=col, right_index=True, how='left')\n",
    "    ref = train.groupby(col)['visitors'].mean().to_frame(name=col+'_target')\n",
    "    test = pd.merge(left=test, right=ref, left_on=col, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val ratio: 15.10%\n"
     ]
    }
   ],
   "source": [
    "val_id = ((train.year == 2017) & (train.month >= 3))\n",
    "print('val ratio:', f'{len(train.loc[val_id])/len(train):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encode('air_store_id')\n",
    "target_encode('air_genre_name')\n",
    "target_encode('air_area_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col = [c for c in train if c not in ['id', 'air_store_id', 'air_area_name', 'air_genre_name', 'visit_date','visitors']]\n",
    "if not log1p_beforehand:\n",
    "    train['visitors'] = np.log1p(train['visitors'])\n",
    "X_trn = train.loc[~val_id, col]\n",
    "y_trn = train.loc[~val_id, 'visitors'].values\n",
    "X_val = train.loc[val_id, col]\n",
    "y_val = train.loc[val_id, 'visitors'].values\n",
    "X_all = train[col]\n",
    "y_all = train['visitors'].values\n",
    "X_tst = test[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'boosting_type': 'gbdt',\n",
    "#     'num_leaves': 70,\n",
    "#     'n_estimators': 500,\n",
    "#     'learning_rate': 0.05,\n",
    "#     'random_state': 77,\n",
    "#     'max_depth': 6,\n",
    "#     'min_child_samples': 50,\n",
    "#     'colsample_bytree': 0.8,\n",
    "#     'subsample': 0.8\n",
    "# }\n",
    "params = {'colsample_bytree': 0.8, 'learning_rate': 0.05, \n",
    " 'min_child_samples': 50, 'n_estimators': 600, 'num_leaves': 70, 'random_state': 77, 'subsample': 1.0}\n",
    "lgb = LGBMRegressor(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "       learning_rate=0.05, max_depth=-1, min_child_samples=50,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=600,\n",
       "       n_jobs=-1, num_leaves=70, objective=None, random_state=77,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.fit(X_trn, y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.4581, val: 0.4855\n",
      "params =  {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'min_child_samples': 50, 'n_estimators': 600, 'num_leaves': 70, 'random_state': 77, 'subsample': 1.0}\n",
      "cols =  ['dow', 'year', 'month', 'day_of_week', 'holiday_flg', 'min_visitors', 'mean_visitors', 'median_visitors', 'max_visitors', 'count_observations', 'latitude', 'longitude', 'air_genre_name0', 'air_area_name0', 'air_genre_name1', 'air_area_name1', 'air_genre_name2', 'air_area_name2', 'air_genre_name3', 'air_area_name3', 'air_genre_name4', 'air_area_name4', 'air_genre_name5', 'air_area_name5', 'air_genre_name6', 'air_area_name6', 'air_genre_name7', 'air_area_name7', 'air_genre_name8', 'air_area_name8', 'air_genre_name9', 'air_area_name9', 'rs1_x', 'rv1_x', 'rs2_x', 'rv2_x', 'rs1_y', 'rv1_y', 'rs2_y', 'rv2_y', 'total_reserv_sum', 'total_reserv_mean', 'total_reserv_dt_diff_mean', 'date_int', 'var_max_lat', 'var_max_long', 'lon_plus_lat', 'air_store_id2', 'air_store_id_target', 'air_genre_name_target', 'air_area_name_target']\n"
     ]
    }
   ],
   "source": [
    "print(f'train: {RMSLE(y_trn, lgb.predict(X_trn)):.4f}, val: {RMSLE(y_val, lgb.predict(X_val)):.4f}')\n",
    "print('params = ', params)\n",
    "print('cols = ', list(X_trn.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=0.8,\n",
       "       learning_rate=0.05, max_depth=-1, min_child_samples=50,\n",
       "       min_child_weight=0.001, min_split_gain=0.0, n_estimators=600,\n",
       "       n_jobs=-1, num_leaves=70, objective=None, random_state=77,\n",
       "       reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "       subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['visitors'] = np.expm1(lgb.predict(X_tst))\n",
    "filename = '../output/180130_lgb7'\n",
    "test[['id', 'visitors']].to_csv(filename+'.csv.gz', index=False, compression='gzip')\n",
    "test[['id', 'visitors']].to_csv(filename+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32019, 57)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_pickle('train.pkl')\n",
    "test.to_pickle('test.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
